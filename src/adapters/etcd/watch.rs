//! etcd watch operations.
//!
//! Implements Watch per ยง10.1.6.
//! - start_revision=0 requires can_linearize=true
//! - Snapshot-only fallback with watch_semantics=SNAPSHOT_ONLY trailer
//! - Progress notifications generated by committed ticks

use super::kv::{KeyValue, ResponseHeader};
use crate::kpg::state_machine::EventType;
use crate::kpg::watch::WatchFilters;
use serde::{Deserialize, Serialize};

/// Watch request (bidirectional stream message).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum WatchRequest {
    /// Create a new watch.
    CreateRequest(WatchCreateRequest),
    /// Cancel an existing watch.
    CancelRequest(WatchCancelRequest),
    /// Request progress notification.
    ProgressRequest(WatchProgressRequest),
}

/// Watch create request.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WatchCreateRequest {
    /// Key to watch.
    pub key: Vec<u8>,
    /// Range end (exclusive).
    pub range_end: Vec<u8>,
    /// Starting revision (0 for "now").
    pub start_revision: i64,
    /// Send progress notifications.
    pub progress_notify: bool,
    /// Event type filters.
    pub filters: Vec<WatchFilterType>,
    /// Include previous key-value.
    pub prev_kv: bool,
    /// Watch ID (0 for auto-assign).
    pub watch_id: i64,
    /// Fragment large responses.
    pub fragment: bool,
}

/// Watch filter type.
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum WatchFilterType {
    /// Filter out PUT events.
    NoPut,
    /// Filter out DELETE events.
    NoDelete,
}

impl WatchFilterType {
    /// Convert to internal WatchFilters.
    pub fn to_internal(filters: &[WatchFilterType]) -> WatchFilters {
        let mut result = WatchFilters::empty();
        for f in filters {
            match f {
                WatchFilterType::NoPut => result.insert(WatchFilters::NOPUT),
                WatchFilterType::NoDelete => result.insert(WatchFilters::NODELETE),
            }
        }
        result
    }
}

/// Watch cancel request.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WatchCancelRequest {
    /// Watch ID to cancel.
    pub watch_id: i64,
}

/// Watch progress request.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WatchProgressRequest {}

/// Watch response (bidirectional stream message).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WatchResponse {
    /// Response header.
    pub header: ResponseHeader,
    /// Watch ID this response is for.
    pub watch_id: i64,
    /// Whether this is a create response.
    pub created: bool,
    /// Whether this is a cancel response.
    pub canceled: bool,
    /// Compaction revision (if watch was canceled due to compaction).
    pub compact_revision: i64,
    /// Cancel reason.
    pub cancel_reason: String,
    /// Fragment flag.
    pub fragment: bool,
    /// Events in this response.
    pub events: Vec<Event>,
}

/// Watch event.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Event {
    /// Event type.
    pub r#type: EventTypeProto,
    /// Current key-value.
    pub kv: KeyValue,
    /// Previous key-value (if requested).
    pub prev_kv: Option<KeyValue>,
}

/// Event type for protobuf.
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum EventTypeProto {
    Put = 0,
    Delete = 1,
}

impl From<EventType> for EventTypeProto {
    fn from(t: EventType) -> Self {
        match t {
            EventType::Put => EventTypeProto::Put,
            EventType::Delete => EventTypeProto::Delete,
        }
    }
}

impl WatchResponse {
    /// Create a "created" response for a new watch.
    pub fn created(header: ResponseHeader, watch_id: i64) -> Self {
        Self {
            header,
            watch_id,
            created: true,
            canceled: false,
            compact_revision: 0,
            cancel_reason: String::new(),
            fragment: false,
            events: Vec::new(),
        }
    }

    /// Create a "canceled" response.
    pub fn canceled(header: ResponseHeader, watch_id: i64, reason: String) -> Self {
        Self {
            header,
            watch_id,
            created: false,
            canceled: true,
            compact_revision: 0,
            cancel_reason: reason,
            fragment: false,
            events: Vec::new(),
        }
    }

    /// Create an events response.
    pub fn events(header: ResponseHeader, watch_id: i64, events: Vec<Event>) -> Self {
        Self {
            header,
            watch_id,
            created: false,
            canceled: false,
            compact_revision: 0,
            cancel_reason: String::new(),
            fragment: false,
            events,
        }
    }

    /// Create a progress notification response.
    pub fn progress(header: ResponseHeader, watch_id: i64) -> Self {
        Self {
            header,
            watch_id,
            created: false,
            canceled: false,
            compact_revision: 0,
            cancel_reason: String::new(),
            fragment: false,
            events: Vec::new(),
        }
    }

    /// Create a compaction error response.
    pub fn compacted(header: ResponseHeader, watch_id: i64, compact_revision: i64) -> Self {
        Self {
            header,
            watch_id,
            created: false,
            canceled: true,
            compact_revision,
            cancel_reason: format!(
                "required revision has been compacted; current compaction floor is {}",
                compact_revision
            ),
            fragment: false,
            events: Vec::new(),
        }
    }
}

// ============================================================================
// Watch Stream Manager (Tasks 111-122)
// ============================================================================

use crate::core::error::{LatticeError, LatticeResult, LinearizabilityFailureReason};
use std::collections::HashMap;
use std::sync::atomic::{AtomicI64, AtomicU64, Ordering};
use std::sync::RwLock;

/// Watch stream configuration.
#[derive(Debug, Clone)]
pub struct WatchStreamConfig {
    /// Maximum events per batch.
    pub max_batch_size: usize,
    /// Maximum watch streams per connection.
    pub max_watches_per_connection: usize,
    /// Progress notification interval in milliseconds (0 to disable).
    pub progress_interval_ms: u64,
    /// Whether to allow snapshot-only fallback.
    pub allow_snapshot_fallback: bool,
}

impl Default for WatchStreamConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 1000,
            max_watches_per_connection: 1000,
            progress_interval_ms: 10_000,
            allow_snapshot_fallback: true,
        }
    }
}

/// Watch stream state for a single watch.
#[derive(Debug, Clone)]
pub struct WatchStreamState {
    /// Watch ID.
    pub watch_id: i64,
    /// Key being watched.
    pub key: Vec<u8>,
    /// Range end (exclusive).
    pub range_end: Vec<u8>,
    /// Last sent revision (persisted for reconnection).
    pub last_sent_revision: i64,
    /// Active filters.
    pub filters: WatchFilters,
    /// Include previous key-value.
    pub prev_kv: bool,
    /// Send progress notifications.
    pub progress_notify: bool,
    /// Whether this watch uses snapshot-only semantics.
    pub snapshot_only: bool,
    /// Fragment large responses.
    pub fragment: bool,
}

impl WatchStreamState {
    /// Create a new watch stream state.
    pub fn new(watch_id: i64, req: &WatchCreateRequest, start_revision: i64) -> Self {
        Self {
            watch_id,
            key: req.key.clone(),
            range_end: req.range_end.clone(),
            last_sent_revision: start_revision,
            filters: WatchFilterType::to_internal(&req.filters),
            prev_kv: req.prev_kv,
            progress_notify: req.progress_notify,
            snapshot_only: false,
            fragment: req.fragment,
        }
    }

    /// Check if a key matches this watch's key range.
    pub fn matches_key(&self, key: &[u8]) -> bool {
        if self.range_end.is_empty() {
            // Single key watch
            key == self.key
        } else if self.range_end == [0] {
            // Prefix watch (range_end = [0] means all keys with prefix)
            key.starts_with(&self.key)
        } else {
            // Range watch
            key >= self.key.as_slice() && key < self.range_end.as_slice()
        }
    }

    /// Check if an event passes the filters.
    pub fn passes_filter(&self, event_type: EventType) -> bool {
        match event_type {
            EventType::Put => !self.filters.contains(WatchFilters::NOPUT),
            EventType::Delete => !self.filters.contains(WatchFilters::NODELETE),
        }
    }

    /// Update last sent revision.
    pub fn update_last_sent(&mut self, revision: i64) {
        if revision > self.last_sent_revision {
            self.last_sent_revision = revision;
        }
    }
}

/// Watch stream semantics indicator.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum WatchSemantics {
    /// Full linearizable watch semantics.
    Linearizable,
    /// Snapshot-only semantics (degraded).
    SnapshotOnly,
}

/// Result of watch creation.
#[derive(Debug)]
pub struct WatchCreateResult {
    /// Assigned watch ID.
    pub watch_id: i64,
    /// Starting revision.
    pub start_revision: i64,
    /// Watch semantics.
    pub semantics: WatchSemantics,
    /// Creation response to send.
    pub response: WatchResponse,
}

/// Watch stream manager.
///
/// Manages watch streams for a connection including creation,
/// event delivery, and cancellation.
pub struct WatchStreamManager {
    /// Configuration.
    config: WatchStreamConfig,
    /// Active watches indexed by watch ID.
    watches: RwLock<HashMap<i64, WatchStreamState>>,
    /// Next watch ID.
    next_watch_id: AtomicI64,
    /// Current compaction floor.
    compaction_floor: AtomicI64,
    /// Watch statistics.
    stats: WatchStreamStats,
}

/// Watch stream statistics.
struct WatchStreamStats {
    /// Total watches created.
    watches_created: AtomicU64,
    /// Total watches canceled.
    watches_canceled: AtomicU64,
    /// Total events delivered.
    events_delivered: AtomicU64,
    /// Total progress notifications.
    progress_notifications: AtomicU64,
}

impl WatchStreamStats {
    fn new() -> Self {
        Self {
            watches_created: AtomicU64::new(0),
            watches_canceled: AtomicU64::new(0),
            events_delivered: AtomicU64::new(0),
            progress_notifications: AtomicU64::new(0),
        }
    }
}

impl WatchStreamManager {
    /// Create a new watch stream manager.
    pub fn new(config: WatchStreamConfig) -> Self {
        Self {
            config,
            watches: RwLock::new(HashMap::new()),
            next_watch_id: AtomicI64::new(1),
            compaction_floor: AtomicI64::new(0),
            stats: WatchStreamStats::new(),
        }
    }

    /// Get the number of active watches.
    pub fn active_watch_count(&self) -> usize {
        self.watches.read().unwrap().len()
    }

    /// Create a new watch.
    ///
    /// Handles start_revision=0 semantics (requires can_linearize),
    /// explicit start_revision validation, and snapshot-only fallback.
    pub fn create_watch(
        &self,
        req: &WatchCreateRequest,
        current_revision: i64,
        can_linearize: bool,
        header: ResponseHeader,
    ) -> LatticeResult<WatchCreateResult> {
        // Check max watches limit
        {
            let watches = self.watches.read().unwrap();
            if watches.len() >= self.config.max_watches_per_connection {
                return Err(LatticeError::InvalidRequest {
                    message: format!(
                        "max watches per connection exceeded ({})",
                        self.config.max_watches_per_connection
                    ),
                });
            }
        }

        // Determine watch ID
        let watch_id = if req.watch_id != 0 {
            req.watch_id
        } else {
            self.next_watch_id.fetch_add(1, Ordering::Relaxed)
        };

        // Determine start revision and semantics
        let (start_revision, semantics) = if req.start_revision == 0 {
            // "Now" semantics - requires linearizability
            if can_linearize {
                (current_revision, WatchSemantics::Linearizable)
            } else if self.config.allow_snapshot_fallback {
                // Fallback to snapshot-only
                (current_revision, WatchSemantics::SnapshotOnly)
            } else {
                // Fail closed
                return Err(LatticeError::linearizability_unavailable(
                    LinearizabilityFailureReason::NotLeader,
                ));
            }
        } else {
            // Explicit start revision
            let compaction_floor = self.compaction_floor.load(Ordering::Acquire);
            if req.start_revision < compaction_floor {
                return Err(LatticeError::RevisionCompacted {
                    revision: req.start_revision as u64,
                    compaction_floor: compaction_floor as u64,
                });
            }
            (req.start_revision, WatchSemantics::Linearizable)
        };

        // Create watch state
        let mut state = WatchStreamState::new(watch_id, req, start_revision);
        state.snapshot_only = semantics == WatchSemantics::SnapshotOnly;

        // Register watch
        {
            let mut watches = self.watches.write().unwrap();
            watches.insert(watch_id, state);
        }

        self.stats.watches_created.fetch_add(1, Ordering::Relaxed);

        // Create response
        let response = WatchResponse::created(header, watch_id);

        Ok(WatchCreateResult {
            watch_id,
            start_revision,
            semantics,
            response,
        })
    }

    /// Cancel a watch.
    pub fn cancel_watch(&self, watch_id: i64, header: ResponseHeader) -> Option<WatchResponse> {
        let removed = self.watches.write().unwrap().remove(&watch_id);

        if removed.is_some() {
            self.stats.watches_canceled.fetch_add(1, Ordering::Relaxed);
            Some(WatchResponse::canceled(
                header,
                watch_id,
                "canceled by request".to_string(),
            ))
        } else {
            None
        }
    }

    /// Get a watch state.
    pub fn get_watch(&self, watch_id: i64) -> Option<WatchStreamState> {
        self.watches.read().unwrap().get(&watch_id).cloned()
    }

    /// Update compaction floor.
    ///
    /// Cancels any watches that have fallen behind the compaction floor.
    pub fn update_compaction_floor(
        &self,
        new_floor: i64,
        header: ResponseHeader,
    ) -> Vec<WatchResponse> {
        let old_floor = self.compaction_floor.swap(new_floor, Ordering::AcqRel);

        if new_floor <= old_floor {
            return Vec::new();
        }

        // Find and cancel compacted watches
        let mut compacted = Vec::new();
        {
            let mut watches = self.watches.write().unwrap();
            let to_cancel: Vec<i64> = watches
                .iter()
                .filter(|(_, state)| state.last_sent_revision < new_floor)
                .map(|(id, _)| *id)
                .collect();

            for watch_id in to_cancel {
                if watches.remove(&watch_id).is_some() {
                    self.stats.watches_canceled.fetch_add(1, Ordering::Relaxed);
                    compacted.push(WatchResponse::compacted(
                        header.clone(),
                        watch_id,
                        new_floor,
                    ));
                }
            }
        }

        compacted
    }

    /// Process events for all watches.
    ///
    /// Returns a list of responses to send, batched by watch ID.
    pub fn process_events(
        &self,
        events: &[WatchableEvent],
        header: ResponseHeader,
    ) -> Vec<WatchResponse> {
        let mut responses = Vec::new();
        let mut pending: HashMap<i64, Vec<Event>> = HashMap::new();

        // Match events to watches
        {
            let watches = self.watches.read().unwrap();
            for event in events {
                for (watch_id, state) in watches.iter() {
                    if !state.matches_key(&event.key) {
                        continue;
                    }
                    if !state.passes_filter(event.event_type) {
                        continue;
                    }
                    if event.mod_revision <= state.last_sent_revision {
                        continue;
                    }

                    let watch_event = Event {
                        r#type: event.event_type.into(),
                        kv: event.kv.clone(),
                        prev_kv: if state.prev_kv {
                            event.prev_kv.clone()
                        } else {
                            None
                        },
                    };

                    pending.entry(*watch_id).or_default().push(watch_event);
                }
            }
        }

        // Create batched responses
        for (watch_id, watch_events) in pending {
            // Batch events according to config
            for chunk in watch_events.chunks(self.config.max_batch_size) {
                let response = WatchResponse::events(header.clone(), watch_id, chunk.to_vec());
                responses.push(response);
            }

            // Update last sent revision
            if let Some(last_event) = watch_events.last() {
                if let Some(state) = self.watches.write().unwrap().get_mut(&watch_id) {
                    state.update_last_sent(last_event.kv.mod_revision);
                }
            }

            self.stats
                .events_delivered
                .fetch_add(watch_events.len() as u64, Ordering::Relaxed);
        }

        responses
    }

    /// Generate progress notifications for all watches with progress_notify=true.
    pub fn generate_progress(&self, header: ResponseHeader) -> Vec<WatchResponse> {
        let mut responses = Vec::new();

        let watches = self.watches.read().unwrap();
        for (watch_id, state) in watches.iter() {
            if state.progress_notify {
                responses.push(WatchResponse::progress(header.clone(), *watch_id));
            }
        }

        self.stats
            .progress_notifications
            .fetch_add(responses.len() as u64, Ordering::Relaxed);

        responses
    }

    /// Resume a watch from a previous last_sent_revision.
    ///
    /// Used for watch reconnection.
    pub fn resume_watch(
        &self,
        req: &WatchCreateRequest,
        last_sent_revision: i64,
        header: ResponseHeader,
    ) -> LatticeResult<WatchCreateResult> {
        // Check compaction
        let compaction_floor = self.compaction_floor.load(Ordering::Acquire);
        if last_sent_revision < compaction_floor {
            return Err(LatticeError::RevisionCompacted {
                revision: last_sent_revision as u64,
                compaction_floor: compaction_floor as u64,
            });
        }

        // Create watch from saved position
        let watch_id = if req.watch_id != 0 {
            req.watch_id
        } else {
            self.next_watch_id.fetch_add(1, Ordering::Relaxed)
        };

        let state = WatchStreamState::new(watch_id, req, last_sent_revision);

        {
            let mut watches = self.watches.write().unwrap();
            watches.insert(watch_id, state);
        }

        self.stats.watches_created.fetch_add(1, Ordering::Relaxed);

        let response = WatchResponse::created(header, watch_id);

        Ok(WatchCreateResult {
            watch_id,
            start_revision: last_sent_revision,
            semantics: WatchSemantics::Linearizable,
            response,
        })
    }

    /// Get watch metrics.
    pub fn metrics(&self) -> WatchStreamMetrics {
        WatchStreamMetrics {
            active_streams: self.watches.read().unwrap().len(),
            watches_created: self.stats.watches_created.load(Ordering::Relaxed),
            watches_canceled: self.stats.watches_canceled.load(Ordering::Relaxed),
            events_delivered: self.stats.events_delivered.load(Ordering::Relaxed),
            progress_notifications: self.stats.progress_notifications.load(Ordering::Relaxed),
        }
    }
}

/// Event that can be watched.
#[derive(Debug, Clone)]
pub struct WatchableEvent {
    /// Key.
    pub key: Vec<u8>,
    /// Event type.
    pub event_type: EventType,
    /// Mod revision of this event.
    pub mod_revision: i64,
    /// Current key-value.
    pub kv: KeyValue,
    /// Previous key-value (if available).
    pub prev_kv: Option<KeyValue>,
}

/// Watch stream metrics.
#[derive(Debug, Clone)]
pub struct WatchStreamMetrics {
    /// Number of active watch streams.
    pub active_streams: usize,
    /// Total watches created.
    pub watches_created: u64,
    /// Total watches canceled.
    pub watches_canceled: u64,
    /// Total events delivered.
    pub events_delivered: u64,
    /// Total progress notifications.
    pub progress_notifications: u64,
}

/// Key range for watch registration.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum WatchKeyRange {
    /// Single key.
    SingleKey(Vec<u8>),
    /// Prefix (all keys starting with prefix).
    Prefix(Vec<u8>),
    /// Range [start, end).
    Range { start: Vec<u8>, end: Vec<u8> },
}

impl WatchKeyRange {
    /// Parse a key range from request fields.
    pub fn parse(key: &[u8], range_end: &[u8]) -> Self {
        if range_end.is_empty() {
            Self::SingleKey(key.to_vec())
        } else if range_end == [0] {
            Self::Prefix(key.to_vec())
        } else {
            Self::Range {
                start: key.to_vec(),
                end: range_end.to_vec(),
            }
        }
    }

    /// Check if a key matches this range.
    pub fn matches(&self, key: &[u8]) -> bool {
        match self {
            Self::SingleKey(k) => key == k,
            Self::Prefix(p) => key.starts_with(p),
            Self::Range { start, end } => key >= start.as_slice() && key < end.as_slice(),
        }
    }

    /// Convert to key and range_end fields.
    pub fn to_fields(&self) -> (Vec<u8>, Vec<u8>) {
        match self {
            Self::SingleKey(k) => (k.clone(), Vec::new()),
            Self::Prefix(p) => (p.clone(), vec![0]),
            Self::Range { start, end } => (start.clone(), end.clone()),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn make_header(revision: i64) -> ResponseHeader {
        ResponseHeader {
            cluster_id: 1,
            member_id: 1,
            revision,
            raft_term: 1,
        }
    }

    fn make_kv(key: &[u8], value: &[u8], mod_revision: i64) -> KeyValue {
        KeyValue {
            key: key.to_vec(),
            create_revision: 1,
            mod_revision,
            version: 1,
            value: value.to_vec(),
            lease: 0,
        }
    }

    #[test]
    fn test_watch_key_range_single() {
        let range = WatchKeyRange::parse(b"key1", b"");
        assert!(range.matches(b"key1"));
        assert!(!range.matches(b"key2"));
    }

    #[test]
    fn test_watch_key_range_prefix() {
        let range = WatchKeyRange::parse(b"/foo/", &[0]);
        assert!(range.matches(b"/foo/bar"));
        assert!(range.matches(b"/foo/baz"));
        assert!(!range.matches(b"/bar/foo"));
    }

    #[test]
    fn test_watch_key_range_range() {
        let range = WatchKeyRange::parse(b"a", b"d");
        assert!(range.matches(b"a"));
        assert!(range.matches(b"b"));
        assert!(range.matches(b"c"));
        assert!(!range.matches(b"d"));
        assert!(!range.matches(b"e"));
    }

    #[test]
    fn test_watch_stream_state_matches() {
        let req = WatchCreateRequest {
            key: b"/prefix/".to_vec(),
            range_end: vec![0], // Prefix watch
            start_revision: 0,
            progress_notify: false,
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let state = WatchStreamState::new(1, &req, 100);
        assert!(state.matches_key(b"/prefix/key1"));
        assert!(state.matches_key(b"/prefix/key2"));
        assert!(!state.matches_key(b"/other/key1"));
    }

    #[test]
    fn test_watch_stream_state_filter() {
        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 0,
            progress_notify: false,
            filters: vec![WatchFilterType::NoDelete],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let state = WatchStreamState::new(1, &req, 100);
        assert!(state.passes_filter(EventType::Put));
        assert!(!state.passes_filter(EventType::Delete));
    }

    #[test]
    fn test_watch_stream_manager_create() {
        let manager = WatchStreamManager::new(WatchStreamConfig::default());

        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 0,
            progress_notify: false,
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let result = manager
            .create_watch(&req, 100, true, make_header(100))
            .expect("should create watch");

        assert_eq!(result.start_revision, 100);
        assert_eq!(result.semantics, WatchSemantics::Linearizable);
        assert!(result.response.created);
        assert_eq!(manager.active_watch_count(), 1);
    }

    #[test]
    fn test_watch_stream_manager_snapshot_fallback() {
        let manager = WatchStreamManager::new(WatchStreamConfig::default());

        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 0, // "now" semantics
            progress_notify: false,
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let result = manager
            .create_watch(&req, 100, false, make_header(100)) // can_linearize = false
            .expect("should create watch with fallback");

        assert_eq!(result.semantics, WatchSemantics::SnapshotOnly);
    }

    #[test]
    fn test_watch_stream_manager_cancel() {
        let manager = WatchStreamManager::new(WatchStreamConfig::default());

        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 0,
            progress_notify: false,
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let result = manager
            .create_watch(&req, 100, true, make_header(100))
            .unwrap();
        assert_eq!(manager.active_watch_count(), 1);

        let cancel_resp = manager.cancel_watch(result.watch_id, make_header(100));
        assert!(cancel_resp.is_some());
        assert!(cancel_resp.unwrap().canceled);
        assert_eq!(manager.active_watch_count(), 0);
    }

    #[test]
    fn test_watch_stream_manager_process_events() {
        let manager = WatchStreamManager::new(WatchStreamConfig::default());

        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 0,
            progress_notify: false,
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let result = manager
            .create_watch(&req, 100, true, make_header(100))
            .unwrap();

        let events = vec![WatchableEvent {
            key: b"key".to_vec(),
            event_type: EventType::Put,
            mod_revision: 101,
            kv: make_kv(b"key", b"value", 101),
            prev_kv: None,
        }];

        let responses = manager.process_events(&events, make_header(101));
        assert_eq!(responses.len(), 1);
        assert_eq!(responses[0].watch_id, result.watch_id);
        assert_eq!(responses[0].events.len(), 1);
    }

    #[test]
    fn test_watch_stream_manager_compaction() {
        let manager = WatchStreamManager::new(WatchStreamConfig::default());

        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 50,
            progress_notify: false,
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let _ = manager
            .create_watch(&req, 100, true, make_header(100))
            .unwrap();
        assert_eq!(manager.active_watch_count(), 1);

        // Compact past the watch's position
        let compacted = manager.update_compaction_floor(75, make_header(100));
        assert_eq!(compacted.len(), 1);
        assert!(compacted[0].canceled);
        assert_eq!(compacted[0].compact_revision, 75);
        assert_eq!(manager.active_watch_count(), 0);
    }

    #[test]
    fn test_watch_stream_manager_progress() {
        let manager = WatchStreamManager::new(WatchStreamConfig::default());

        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 0,
            progress_notify: true, // Enable progress notifications
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let _ = manager
            .create_watch(&req, 100, true, make_header(100))
            .unwrap();

        let progress = manager.generate_progress(make_header(105));
        assert_eq!(progress.len(), 1);
        assert_eq!(progress[0].events.len(), 0);
    }

    #[test]
    fn test_watch_stream_metrics() {
        let manager = WatchStreamManager::new(WatchStreamConfig::default());

        let req = WatchCreateRequest {
            key: b"key".to_vec(),
            range_end: vec![],
            start_revision: 0,
            progress_notify: false,
            filters: vec![],
            prev_kv: false,
            watch_id: 0,
            fragment: false,
        };

        let result = manager
            .create_watch(&req, 100, true, make_header(100))
            .unwrap();
        manager.cancel_watch(result.watch_id, make_header(100));

        let metrics = manager.metrics();
        assert_eq!(metrics.watches_created, 1);
        assert_eq!(metrics.watches_canceled, 1);
    }
}
